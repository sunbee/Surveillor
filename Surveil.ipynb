{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Tensorflow Lite for predictions with an off-the-shelf deep-learning model. The model architecture and weights are pre-determined. Use the ```Interpreter``` class from ```tflite_runtime``` that has only this class. \n",
    "1. Find the download link [here](https://dl.google.com/coral/python/tflite_runtime-1.14.0-cp37-cp37m-linux_armv7l.whl) for Python 3.7. \n",
    "2. Install with pip3 installer, pointing to the local copy of the '.whl' file. \n",
    "3. Verify by opening python kernel and typing ```from tflite_runtime.interpreter import Interpreter```.\n",
    "\n",
    "Now, proceed with the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflite_runtime.interpreter import Interpreter\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from picamera import PiCamera\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```Interpreter``` class makes predictions with a pre-trained model that is in the format required for Tensorflow Lite. These models conform to an architecture that is a deep-learning staple, such as, MobileNet. However, this need not be very restrictive. It is possible to obtain such a model from a service like Google's [Teachable Machine](https://teachablemachine.withgoogle.com) that allows some tweaking the model based on neural transfer learning.\n",
    "\n",
    "For this exercise, download the [MobileNet](https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_1.0_224_quant_and_labels.zip) Convolutional Neural Network (CNN). The zip archive has two files as follows: (1.) The set of weights (2.) The category labels for classification. The weights are represented with 8-bits each so that the model trades off precision for lower storage, thus taking up less memory for computation.\n",
    "\n",
    "Let's get set up to load the model components extracted from the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/pi/Downloads/MobileNet/mobilenet_v1_1.0_224_quant.tflite'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_source = '/home/pi/Downloads/MobileNet'\n",
    "\n",
    "path2model = os.path.join(data_source, 'mobilenet_v1_1.0_224_quant.tflite')\n",
    "path2labels = os.path.join(data_source, 'labels_mobilenet_quant_v1_224.txt')\n",
    "\n",
    "path2labels\n",
    "path2model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the category labels and examine the top 5 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['background',\n",
       " 'tench',\n",
       " 'goldfish',\n",
       " 'great white shark',\n",
       " 'tiger shark',\n",
       " 'hammerhead',\n",
       " 'electric ray']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(path2labels) as text_labels:\n",
    "    labels = text_labels.readlines()\n",
    "labels = [label.rstrip() for label in labels]\n",
    "labels[0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the weights and allocate tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = Interpreter(path2model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "height = input_details[0][\"shape\"][1]\n",
    "width = input_details[0][\"shape\"][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model expects an input image in certain shape. Let's examine. We will get our images into the shape required by the model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use an input image of size 224 x 224.\n"
     ]
    }
   ],
   "source": [
    "print(\"Use an input image of size {} x {}.\".format(height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the dimensions, let's prepare an image to use in classification. We will take a picture with picamera first. Let's set up the picamera. Note that the code in the cell (below) must be run only once or otherwise, restart the kernel and run the notebook starting over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_stream = BytesIO()\n",
    "\n",
    "eye = PiCamera()\n",
    "eye.rotation = 180\n",
    "eye.resolution = (300, 300)\n",
    "\n",
    "time.sleep(2)  # Warm up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capture an image from the byte stream and resize for feeding to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye.capture(eye_stream, 'jpeg')\n",
    "eye_stream.seek(0)\n",
    "snap = Image.open(eye_stream).convert('RGB').resize((height, width))\n",
    "snap.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, set the tensor to feed the image to the deep-learning model for image classification. Call the ```invoke()``` method to propagate the input through the layers of the CNN in feed-forward mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "im2classify = np.expand_dims(snap, axis=0)\n",
    "interpreter.set_tensor(input_details[0][\"index\"], im2classify)\n",
    "interpreter.invoke()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With calculations complete, process the output to extract the *k*th best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Polaroid camera', 0.21484375)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "output = np.squeeze(interpreter.get_tensor(output_details[0][\"index\"]))\n",
    "\n",
    "scale, zero_point = output_details[0][\"quantization\"]\n",
    "output = scale * (output - zero_point)\n",
    "\n",
    "top = 1\n",
    "ordered = np.argpartition(-output, top)\n",
    "[(labels[i], output[i]) for i in ordered[:top]][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package the code in functions for use in the envisaged computer vision application. Ref. docstrings for details about each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(path2labels):\n",
    "    \"\"\"\n",
    "    Load the text file with the canned model into a list.\n",
    "    Args\n",
    "    - path2labels is the location (string) of the local copy of the file containing classification labels.\n",
    "    Returns a list of classification labels.\n",
    "    \"\"\"\n",
    "    with open(path2labels) as text_labels:\n",
    "        labels = text_labels.readlines()\n",
    "    return [label.rstrip() for label in labels]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_input_tensor(interpreter, snap):\n",
    "    \"\"\"\n",
    "    Convert the image from camera to a tensor for feeding to the convolutional neural network.\n",
    "    Args\n",
    "    - interpreter is the object of class Interpreter from Tensorflow Lite runtime \n",
    "    - snap is the open image from the picamera\n",
    "    Returns None\n",
    "    Note that Interpreter is the sole class in the Tensorflow Lite runtime.\n",
    "    \"\"\"\n",
    "    im2classify = np.expand_dims(snap, axis=0)\n",
    "    interpreter.set_tensor(interpreter.get_input_details()[0][\"index\"], im2classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_topkth(interpreter, topkth):\n",
    "    \"\"\"\n",
    "    Processes the output to report the top kth match with success probability.\n",
    "    Args\n",
    "    - interpreter is the object of class Interpreter from Tensorflow Lite runtime\n",
    "    - topkth is an integer specifying the desired rank of the match\n",
    "    Returns a tuple with the match ID and raw success probability.\n",
    "    \"\"\"\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    output = np.squeeze(interpreter.get_tensor(output_details[0][\"index\"]))\n",
    "    scale, zero_point = output_details[0][\"quantization\"]\n",
    "    output = scale * (output - zero_point)\n",
    "\n",
    "    ordered = np.argpartition(-output, topkth)\n",
    "    return [(i, output[i]) for i in ordered[:topkth]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_me(interpreter, snap, topkth=1):\n",
    "    set_input_tensor(interpreter, snap)\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    return report_topkth(interpreter, topkth)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 'Polaroid camera' with probability of 21%.\n"
     ]
    }
   ],
   "source": [
    "labels = load_labels(path2labels)\n",
    "\n",
    "classID, classProbability = classify_me(interpreter, snap)\n",
    "\n",
    "print(\"Detected '{}' with probability of {}%.\".format(labels[classID], int(classProbability*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We have tested an approach for offline object detection on Raspberry Pi. The online approach based on calling a web service requires WiFi whereas the offline approach does not. We have overcome issues arising out of compatibility of the Raspberry Pi's the ARM architecture with key computational components as follows:\n",
    "1. Use PIL for working with image data instead of the pesky OpenCV.\n",
    "2. Use Tensorflow Lite runtime instead of the full-fledged Tensorflow library. \n",
    "3. Use Teachable Machine to tweak thedeep-learning models that apply standard CNN architectures if needed.\n",
    "We are now poised to add computer vision features to projects such as the smart-seat to prevent infant deaths in a \"hot car\" and the surveillance system for home security."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1.] A [guide](https://www.digikey.com/en/maker/projects/how-to-perform-object-detection-with-tensorflow-lite-on-raspberry-pi/b929e1519c7c43d5b2c6f89984883588) to object detection with Tensorflow Lite on Raspberry Pi from DigiKey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 32-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
